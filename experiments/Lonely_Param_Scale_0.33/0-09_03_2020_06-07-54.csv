Experiment 1 ,09_03_2020_06-07-54

Data shape and running
input_shape ,"(28, 28)"
output_shape ,10
scaling ,255.0
epochs ,5
max_runtime,1800
dataset_percentage,0.1
dataset,mnist

Hyper parameters
activation_function ,Activation.relu
initial_max_nodes ,50
loss_function ,Loss.sparse_categorical_crossentropy
optimizer ,Optimizer.Adam

GA parameters
population_size ,10
mating_pool ,10
mutation_rate,0.2

OUTPUT
generation_no,params_no,neurons_no,accuracy,loss
0,7165,9,0.876,0.41105840158462525
1,7960,10,0.888,0.38344118547439576
2,7165,9,0.883,0.42677051305770874
3,5575,7,0.836,0.5340555248260498
4,3985,5,0.722,0.8754836673736572
5,5575,7,0.849,0.5018569679260254
6,3985,5,0.764,0.7556954708099365
7,3985,5,0.802,0.6828418126106263
8,3190,4,0.756,0.7401678657531738
9,3190,4,0.733,0.9142097177505493
10,3985,5,0.766,0.7337287397384643
11,3190,4,0.733,0.9059586000442504
12,3985,5,0.773,0.7877652215957641
13,3190,4,0.771,0.7978527717590332
14,2395,3,0.642,1.1388868312835694
15,2395,3,0.678,1.0433206648826598
16,3985,5,0.817,0.6220251259803772
17,3190,4,0.767,0.755072874546051
18,3190,4,0.75,0.8169903087615967
19,3190,4,0.713,0.8892416648864746
20,2395,3,0.657,1.1462587032318114
21,2395,3,0.658,1.1014300050735473
22,2395,3,0.58,1.2552986316680907
23,2395,3,0.64,1.2074937286376952
24,2395,3,0.593,1.1898416728973389
25,2395,3,0.62,1.131521661758423
26,2395,3,0.671,1.0557584991455078
27,2395,3,0.669,1.1753374500274658
28,2395,3,0.591,1.2622290906906128
29,2395,3,0.612,1.2743683242797852
30,2395,3,0.63,1.135632538795471
31,2395,3,0.639,1.1476719036102294
32,2395,3,0.692,0.9944470720291138
33,2395,3,0.662,1.097884355545044
34,2395,3,0.632,1.2363713741302491
35,2395,3,0.667,1.0842673072814941
36,2395,3,0.576,1.18258349609375
37,2395,3,0.593,1.2105777111053466
38,2395,3,0.615,1.087652014732361
39,2395,3,0.623,1.0561965770721435
40,1600,2,0.495,1.4766213779449462
41,1600,2,0.428,1.5503699359893799
42,1600,2,0.495,1.4571609964370729
43,805,1,0.223,1.9846153240203857
44,805,1,0.251,2.0027105979919435
45,805,1,0.269,1.9144742126464844
46,805,1,0.24,2.007004426956177
47,805,1,0.224,1.9809812602996826
48,805,1,0.295,1.9060001211166382
49,805,1,0.271,1.92598202419281
50,805,1,0.247,1.919859432220459
51,805,1,0.242,1.9996903114318847
52,805,1,0.229,1.919591947555542
53,805,1,0.246,1.9396177139282227
54,805,1,0.266,1.9579673957824708
55,805,1,0.251,1.8944910774230956
56,805,1,0.242,1.9367343254089355
57,805,1,0.242,2.0178080463409422
58,805,1,0.241,2.0133205604553224
59,805,1,0.236,1.9825981979370118
60,805,1,0.262,1.9588646326065065
61,805,1,0.247,1.9122541828155517
62,805,1,0.256,1.930948642730713
63,805,1,0.251,1.9471646461486816
64,805,1,0.231,1.928433790206909
65,805,1,0.257,1.965352825164795
66,805,1,0.261,1.905722767829895
67,805,1,0.255,1.9282478618621826
68,805,1,0.255,1.9245085525512695
69,805,1,0.247,2.0031091194152832
70,805,1,0.248,1.9702044162750245
71,805,1,0.209,1.9420094470977782
72,805,1,0.244,1.9340185089111328
73,805,1,0.243,1.8990062284469604
74,805,1,0.246,1.975235179901123
75,805,1,0.239,1.9888299655914308
76,805,1,0.241,1.9690715084075927
77,805,1,0.247,1.9602083549499512
78,805,1,0.255,1.9516890678405763
79,805,1,0.252,1.9671880140304565
80,805,1,0.244,2.0269652795791626
81,805,1,0.22,1.9469941310882568
82,805,1,0.235,1.9883067474365235
83,805,1,0.251,1.974564458847046
84,805,1,0.239,1.8932375926971436
85,805,1,0.247,1.9376748600006104
86,805,1,0.273,2.038154945373535
87,805,1,0.241,1.986815839767456
88,805,1,0.26,1.9672469520568847
89,805,1,0.243,1.9246445789337159
90,805,1,0.231,1.9687084197998046
91,805,1,0.225,1.8791735897064208
92,805,1,0.24,1.8798726234436036
