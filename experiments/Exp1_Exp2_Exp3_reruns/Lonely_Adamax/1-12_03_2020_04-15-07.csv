Experiment 2 ,12_03_2020_04-15-07

Data shape and running
input_shape ,"(28, 28)"
output_shape ,10
scaling ,255.0
epochs ,5
max_runtime,1800
dataset_percentage,0.1
dataset,mnist

Hyper parameters
activation_function ,Activation.relu
initial_max_nodes ,50
loss_function ,Loss.sparse_categorical_crossentropy
optimizer ,Optimizer.Adamax

GA parameters
population_size ,10
mating_pool ,10
mutation_rate,0.2

OUTPUT
generation_no,params_no,neurons_no,accuracy,loss
0,38965,49,0.883,0.3973110389709473
1,43735,55,0.887,0.38420857620239257
2,32605,41,0.884,0.40259997129440306
3,29425,37,0.888,0.4131399760246277
4,29425,37,0.882,0.41668497467041016
5,45325,57,0.89,0.37630543279647827
6,52480,66,0.892,0.3733580889701843
7,64405,81,0.888,0.37575157070159915
8,38170,48,0.887,0.39583406019210815
9,46120,58,0.895,0.37714181566238403
10,41350,52,0.892,0.3875671181678772
11,50095,63,0.891,0.37055731868743896
12,59635,75,0.896,0.36264968872070313
13,61225,77,0.894,0.3675060486793518
14,65995,83,0.896,0.36642835092544557
15,47710,60,0.892,0.38381368350982664
16,53275,67,0.887,0.3756325907707214
17,38965,49,0.891,0.3847218232154846
18,50890,64,0.889,0.3710556330680847
19,36580,46,0.891,0.40525208473205565
20,43735,55,0.894,0.3843953185081482
21,46120,58,0.885,0.3793349771499634
22,57250,72,0.895,0.37061304473876955
23,27040,34,0.883,0.4096840977668762
24,31015,39,0.878,0.42008367443084715
25,31015,39,0.878,0.420954222202301
26,27835,35,0.879,0.4079550585746765
27,27835,35,0.878,0.4241401972770691
28,27040,34,0.886,0.4135222201347351
29,29425,37,0.879,0.4172410650253296
30,32605,41,0.879,0.41173232889175415
31,26245,33,0.884,0.41899383354187014
32,26245,33,0.863,0.43727512550354003
33,27835,35,0.878,0.42727717304229734
34,28630,36,0.889,0.4116670560836792
35,19090,24,0.873,0.43925808906555175
36,14320,18,0.877,0.4696553802490234
37,15910,20,0.859,0.4923243131637573
38,14320,18,0.869,0.48872847843170164
39,15115,19,0.856,0.5065756521224976
40,15115,19,0.861,0.47923410415649415
41,15115,19,0.867,0.4835072140693665
42,21475,27,0.876,0.4399341402053833
43,18295,23,0.87,0.4568757424354553
44,24655,31,0.869,0.4431875877380371
45,26245,33,0.878,0.4305741024017334
46,28630,36,0.886,0.4016088523864746
47,19885,25,0.874,0.4447687168121338
48,22270,28,0.874,0.4455550622940063
49,26245,33,0.872,0.4433767967224121
50,30220,38,0.881,0.40880030965805053
51,30220,38,0.874,0.411279812335968
52,37375,47,0.884,0.39547568655014037
53,40555,51,0.886,0.39487103939056395
54,15910,20,0.867,0.4889732871055603
55,19885,25,0.868,0.44977936363220217
56,19885,25,0.883,0.43901690769195556
57,19090,24,0.87,0.4575796098709106
58,21475,27,0.866,0.45289442110061645
59,14320,18,0.867,0.5033663415908813
60,15910,20,0.877,0.4547278871536255
61,16705,21,0.868,0.48995953702926637
62,19090,24,0.872,0.46997780990600585
63,11140,14,0.849,0.5386071372032165
64,19090,24,0.875,0.4599451103210449
65,15115,19,0.87,0.4706468181610107
66,14320,18,0.866,0.4793480663299561
67,15910,20,0.864,0.4785498161315918
68,17500,22,0.875,0.4637680788040161
69,20680,26,0.877,0.43372442388534543
70,25450,32,0.88,0.4118705019950867
71,21475,27,0.884,0.43031437540054324
72,10345,13,0.854,0.5470055003166199
73,11140,14,0.858,0.5166920976638794
74,13525,17,0.87,0.4989445939064026
75,15115,19,0.878,0.4688046851158142
76,21475,27,0.874,0.4462835183143616
77,24655,31,0.874,0.43716773653030394
78,32605,41,0.885,0.40774309539794923
79,24655,31,0.879,0.4305362434387207
80,27835,35,0.886,0.4077258925437927
81,28630,36,0.869,0.42906096696853635
82,32605,41,0.879,0.4171499090194702
83,23065,29,0.885,0.42446968841552735
84,24655,31,0.874,0.44237001466751097
85,15115,19,0.855,0.49290142917633056
86,16705,21,0.866,0.48328510189056395
87,16705,21,0.866,0.4895827989578247
88,13525,17,0.857,0.47842303562164307
89,16705,21,0.871,0.46055391788482664
