Experiment 2 ,09_03_2020_06-37-54

Data shape and running
input_shape ,"(28, 28)"
output_shape ,10
scaling ,255.0
epochs ,5
max_runtime,1800
dataset_percentage,0.1
dataset,mnist

Hyper parameters
activation_function ,Activation.relu
initial_max_nodes ,50
loss_function ,Loss.sparse_categorical_crossentropy
optimizer ,Optimizer.Adam

GA parameters
population_size ,10
mating_pool ,10
mutation_rate,0.2

OUTPUT
generation_no,params_no,neurons_no,accuracy,loss
0,5575,7,0.847,0.4913800458908081
1,5575,7,0.847,0.5051785569190979
2,4780,6,0.8,0.6684742527008056
3,4780,6,0.821,0.5511394910812378
4,3985,5,0.824,0.5757444548606873
5,3190,4,0.773,0.7502420206069946
6,3985,5,0.784,0.7057534818649291
7,3190,4,0.779,0.7949940309524536
8,3985,5,0.797,0.6480204014778137
9,3190,4,0.79,0.7946878309249878
10,3985,5,0.818,0.6231673460006714
11,3190,4,0.757,0.7717293038368225
12,3190,4,0.794,0.7154443249702453
13,3985,5,0.805,0.6758412532806396
14,3985,5,0.787,0.7177971954345703
15,2395,3,0.681,1.1562175493240356
16,3985,5,0.806,0.68428742313385
17,3985,5,0.776,0.7637670331001282
18,3190,4,0.723,0.8376312007904053
19,3985,5,0.78,0.7602269225120545
20,3985,5,0.816,0.6431227836608887
21,3985,5,0.812,0.6501734933853149
22,3985,5,0.785,0.6615381336212158
23,3190,4,0.763,0.8151462745666503
24,3190,4,0.754,0.7797831945419311
25,3190,4,0.752,0.888868863105774
26,3190,4,0.756,0.7802872314453125
27,2395,3,0.636,1.107556757926941
28,2395,3,0.648,1.1032456607818604
29,2395,3,0.658,1.1448429374694824
30,2395,3,0.611,1.1835686302185058
31,2395,3,0.644,1.1306887178421021
32,2395,3,0.631,1.1765479745864869
33,1600,2,0.47,1.5060970478057862
34,1600,2,0.48,1.605611846923828
35,1600,2,0.464,1.5332184581756592
36,1600,2,0.461,1.5811586608886719
37,1600,2,0.438,1.551169584274292
38,1600,2,0.475,1.6318344240188598
39,1600,2,0.421,1.5598077173233031
40,1600,2,0.451,1.4884389114379883
41,1600,2,0.48,1.526877384185791
42,1600,2,0.462,1.5809688081741333
43,1600,2,0.402,1.5147854804992675
44,1600,2,0.5,1.6021481227874756
45,805,1,0.241,2.012522705078125
46,805,1,0.25,1.9616428909301757
47,805,1,0.25,1.9615373334884643
48,805,1,0.264,1.926769296646118
49,805,1,0.221,1.9605672340393066
50,805,1,0.272,1.920001371383667
51,805,1,0.238,1.961813367843628
52,805,1,0.263,1.9308345279693604
53,805,1,0.248,1.9699066200256348
54,805,1,0.271,1.9000403289794923
55,805,1,0.266,1.9056979331970214
56,805,1,0.265,1.8829025592803954
57,805,1,0.227,1.9500232086181641
58,805,1,0.255,1.9499234466552735
59,805,1,0.227,1.939155574798584
60,805,1,0.243,1.8878377056121827
61,805,1,0.234,1.9782103605270385
62,805,1,0.236,1.949714891433716
63,805,1,0.244,1.9472774467468261
64,805,1,0.254,1.9008966293334961
65,805,1,0.254,1.9399635734558105
66,805,1,0.216,1.9507664222717285
67,805,1,0.234,1.9537070407867432
68,805,1,0.214,1.9074475021362305
69,805,1,0.25,2.024674509048462
70,805,1,0.281,1.9630671691894532
71,805,1,0.229,1.9745856170654297
72,805,1,0.233,1.9410967102050782
73,805,1,0.262,1.9920191020965576
74,805,1,0.246,1.9765936698913573
75,805,1,0.256,1.994239990234375
76,805,1,0.254,1.9514011821746826
77,805,1,0.274,1.9650793628692627
78,805,1,0.26,2.0078618421554566
79,805,1,0.245,1.981917091369629
80,805,1,0.225,1.8994604301452638
81,805,1,0.234,1.99881982421875
82,805,1,0.259,1.8954210796356201
83,805,1,0.279,1.9002197151184081
84,805,1,0.262,1.920067958831787
85,805,1,0.254,1.9238647718429565
86,805,1,0.245,2.0255485820770263
87,805,1,0.243,1.9636074600219726
88,805,1,0.217,1.9462948207855224
89,805,1,0.299,1.8947079935073852
90,805,1,0.261,1.9364282608032226
