Experiment 2 ,12_03_2020_00-15-04

Data shape and running
input_shape ,"(28, 28)"
output_shape ,10
scaling ,255.0
epochs ,5
max_runtime,1800
dataset_percentage,0.1
dataset,mnist

Hyper parameters
activation_function ,Activation.elu
initial_max_nodes ,50
loss_function ,Loss.sparse_categorical_crossentropy
optimizer ,Optimizer.Adam

GA parameters
population_size ,10
mating_pool ,10
mutation_rate,0.2

OUTPUT
generation_no,params_no,neurons_no,accuracy,loss
0,28630,36,0.889,0.33260373783111574
1,35785,45,0.895,0.3344281456470489
2,30220,38,0.91,0.32098332023620607
3,40555,51,0.906,0.32393464756011964
4,40555,51,0.901,0.3313114783763885
5,17500,22,0.899,0.3364003636837006
6,13525,17,0.893,0.35611723470687867
7,14320,18,0.902,0.33604418230056765
8,18295,23,0.895,0.3405404496192932
9,14320,18,0.887,0.35703357219696047
10,15910,20,0.886,0.3410648441314697
11,17500,22,0.896,0.3545327038764954
12,19090,24,0.896,0.34369299936294556
13,24655,31,0.892,0.3463350615501404
14,24655,31,0.89,0.34365407514572144
15,24655,31,0.892,0.3462706151008606
16,11935,15,0.887,0.35372123956680296
17,14320,18,0.889,0.35592998600006104
18,14320,18,0.894,0.35380882930755614
19,12730,16,0.888,0.3668404369354248
20,10345,13,0.883,0.3660242290496826
21,12730,16,0.892,0.3477952017784119
22,12730,16,0.89,0.3494698052406311
23,10345,13,0.89,0.3623041639328003
24,10345,13,0.886,0.3675954189300537
25,10345,13,0.879,0.4011610131263733
26,5575,7,0.868,0.5021354823112488
27,4780,6,0.855,0.5549745750427246
28,6370,8,0.863,0.48400077867507935
29,6370,8,0.866,0.4591738965511322
30,7960,10,0.874,0.39770319032669066
31,8755,11,0.879,0.3970569553375244
32,8755,11,0.875,0.41396293020248415
33,4780,6,0.837,0.5191204462051392
34,5575,7,0.829,0.576068395614624
35,5575,7,0.827,0.5396344418525696
36,5575,7,0.85,0.48922091197967527
37,5575,7,0.839,0.5303883666992187
38,6370,8,0.836,0.5238407254219055
39,4780,6,0.827,0.5647949237823486
40,4780,6,0.845,0.5290486564636231
41,4780,6,0.802,0.6419940009117127
42,3190,4,0.718,0.9318823184967041
43,2395,3,0.667,1.1404015808105468
44,2395,3,0.592,1.1607029457092286
45,2395,3,0.658,1.0540684404373168
46,2395,3,0.678,1.0698501615524292
47,2395,3,0.568,1.127431456565857
48,2395,3,0.679,1.038308811187744
49,2395,3,0.686,1.043335205078125
50,2395,3,0.666,1.0984270210266114
51,1600,2,0.492,1.4003479290008545
52,1600,2,0.454,1.4099397583007813
53,1600,2,0.461,1.4430130653381348
54,1600,2,0.476,1.5019398918151856
55,1600,2,0.494,1.460091742515564
56,1600,2,0.464,1.4591380863189698
57,805,1,0.283,1.844981637954712
58,805,1,0.221,1.8745311813354493
59,805,1,0.273,1.859458465576172
60,805,1,0.242,1.8791414394378663
61,805,1,0.268,1.8103508815765381
62,805,1,0.274,1.8674394264221192
63,805,1,0.268,1.8797088766098022
64,805,1,0.288,1.894310905456543
65,805,1,0.255,1.8910605220794678
66,805,1,0.281,1.8173657112121582
67,805,1,0.275,1.8827501850128174
68,805,1,0.256,1.886581537246704
69,805,1,0.294,1.9108995723724365
70,805,1,0.248,1.8320661525726318
71,805,1,0.279,1.9138724060058594
72,805,1,0.22,1.8911310119628906
73,805,1,0.235,1.8876863441467284
74,805,1,0.267,1.919426817893982
75,805,1,0.286,1.7928932600021363
76,805,1,0.28,1.9289546279907226
77,805,1,0.27,1.893983829498291
78,805,1,0.241,1.8066143341064453
79,805,1,0.271,1.8750586652755736
80,805,1,0.256,1.914857759475708
81,805,1,0.29,1.8751497917175293
82,805,1,0.259,1.8733542194366455
83,805,1,0.276,1.9162655906677246
84,805,1,0.264,1.8414863491058349
85,805,1,0.256,1.873345500946045
86,805,1,0.228,1.9018578510284423
87,805,1,0.27,1.8734561443328857
88,805,1,0.269,1.9037082862854005
89,805,1,0.226,1.8618562335968019
90,805,1,0.271,1.8193625984191895
