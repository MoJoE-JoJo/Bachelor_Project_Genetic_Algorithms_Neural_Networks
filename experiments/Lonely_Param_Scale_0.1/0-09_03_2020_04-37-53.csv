Experiment 1 ,09_03_2020_04-37-53

Data shape and running
input_shape ,"(28, 28)"
output_shape ,10
scaling ,255.0
epochs ,5
max_runtime,1800
dataset_percentage,0.1
dataset,mnist

Hyper parameters
activation_function ,Activation.relu
initial_max_nodes ,50
loss_function ,Loss.sparse_categorical_crossentropy
optimizer ,Optimizer.Adam

GA parameters
population_size ,10
mating_pool ,10
mutation_rate,0.2

OUTPUT
generation_no,params_no,neurons_no,accuracy,loss
0,7165,9,0.868,0.4256076502799988
1,5575,7,0.838,0.5334837694168091
2,7165,9,0.859,0.4875736479759216
3,7165,9,0.849,0.5193521466255188
4,7960,10,0.868,0.43984245777130127
5,7960,10,0.882,0.4117616333961487
6,8755,11,0.889,0.3752639498710632
7,7960,10,0.856,0.4515101051330566
8,6370,8,0.858,0.4636411318778992
9,7165,9,0.874,0.4327767233848572
10,6370,8,0.847,0.5006620335578919
11,7960,10,0.883,0.39880833387374875
12,7165,9,0.874,0.43598470973968506
13,6370,8,0.873,0.4636286573410034
14,6370,8,0.871,0.4710364828109741
15,6370,8,0.866,0.4563337450027466
16,6370,8,0.873,0.42611470079422
17,5575,7,0.862,0.46782896900177
18,5575,7,0.832,0.5334109992980957
19,6370,8,0.855,0.49373325443267824
20,4780,6,0.81,0.5858581533432007
21,7165,9,0.87,0.4474296541213989
22,5575,7,0.841,0.49330064725875855
23,5575,7,0.858,0.4817774124145508
24,5575,7,0.845,0.5582327270507812
25,5575,7,0.84,0.49487535667419436
26,4780,6,0.83,0.5769684324264527
27,3985,5,0.785,0.6896594705581665
28,4780,6,0.842,0.56653666639328
29,5575,7,0.83,0.5807115936279297
30,4780,6,0.821,0.5807035565376282
31,4780,6,0.806,0.5925937871932984
32,4780,6,0.844,0.5300108003616333
33,4780,6,0.837,0.5658768920898437
34,3985,5,0.775,0.7175316019058228
35,4780,6,0.813,0.6054116706848145
36,3985,5,0.834,0.7396011476516724
37,3985,5,0.794,0.6989346694946289
38,3985,5,0.827,0.6016944990158081
39,4780,6,0.831,0.5789499526023865
40,4780,6,0.843,0.5567901878356933
41,4780,6,0.808,0.6558563165664673
42,4780,6,0.802,0.6194553489685058
43,5575,7,0.836,0.5949068622589111
44,4780,6,0.827,0.64214635181427
45,3985,5,0.792,0.6708022253513336
46,4780,6,0.837,0.5876602697372436
47,4780,6,0.846,0.5300332536697387
48,3190,4,0.791,0.6872345237731934
49,3985,5,0.781,0.7360173511505127
50,3985,5,0.795,0.6551218156814576
51,3190,4,0.741,0.935313349723816
52,2395,3,0.62,1.1651577043533325
53,2395,3,0.631,1.1617892780303956
54,2395,3,0.648,1.2241524600982665
55,2395,3,0.698,1.0159008617401124
56,2395,3,0.632,1.16844557762146
57,2395,3,0.669,1.0925935020446778
58,2395,3,0.579,1.2331192245483398
59,2395,3,0.633,1.106065538406372
60,2395,3,0.648,1.1599163703918458
61,2395,3,0.687,0.967479076385498
62,2395,3,0.669,1.0521315479278563
63,2395,3,0.69,1.0135421180725097
64,2395,3,0.664,0.9994112339019775
65,1600,2,0.495,1.512441904067993
66,1600,2,0.466,1.4939409132003785
67,1600,2,0.495,1.5616640434265137
68,805,1,0.254,1.9530540924072266
69,805,1,0.238,1.9932670364379883
70,805,1,0.235,2.0720133876800535
71,805,1,0.246,1.998742088317871
72,805,1,0.232,1.9213832473754884
73,805,1,0.306,1.8988898448944092
74,805,1,0.25,1.9802325954437257
75,805,1,0.264,1.9893733158111573
76,805,1,0.243,1.9451368961334228
77,805,1,0.251,1.9757692823410034
78,805,1,0.247,2.013413164138794
79,805,1,0.246,1.9400323143005371
80,805,1,0.247,2.0024685440063474
81,805,1,0.267,1.911811082839966
82,805,1,0.235,1.9146866645812988
83,805,1,0.299,2.01346856880188
84,805,1,0.259,1.9664115829467774
85,805,1,0.293,1.9347220659255981
86,805,1,0.243,2.0136819515228273
87,805,1,0.256,1.962003532409668
88,805,1,0.252,1.983248525619507
89,805,1,0.23,1.9527193908691407
90,805,1,0.24,2.040507257461548
