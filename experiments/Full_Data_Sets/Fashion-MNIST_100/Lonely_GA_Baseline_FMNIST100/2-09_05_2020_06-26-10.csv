Experiment 3 ,09_05_2020_06-26-10

Data shape and running
input_shape ,"(28, 28)"
output_shape ,10
scaling ,255.0
epochs ,5
max_runtime,10800
dataset_percentage,1.0
dataset,fashion_mnist

Hyper parameters
activation_function ,Activation.relu
initial_max_nodes ,50
loss_function ,Loss.sparse_categorical_crossentropy
optimizer ,Optimizer.Adam

GA parameters
population_size ,10
mating_pool ,10
mutation_rate,0.8

OUTPUT
generation_no,params_no,neurons_no,accuracy,loss
0,36580,46,0.8712,0.3599865875840187
1,31810,40,0.862,0.38453761751651766
2,38965,49,0.8707,0.369820702290535
3,40555,51,0.8695,0.36671202148199084
4,37375,47,0.8646,0.3815298625469208
5,41350,52,0.8593,0.3891348815202713
6,45325,57,0.8633,0.3847396559715271
7,45325,57,0.8632,0.38096555576324465
8,46120,58,0.8656,0.3724521587610245
9,44530,56,0.8687,0.3638475968122482
10,38170,48,0.8588,0.38398208656311034
11,53275,67,0.8757,0.34959947149753573
12,57250,72,0.8718,0.3556616847634316
13,65995,83,0.8724,0.353420539522171
14,59635,75,0.8695,0.3673242043375969
15,69970,88,0.8719,0.36280139088630675
16,69970,88,0.8697,0.36384449577331546
17,55660,70,0.8775,0.351091817855835
18,55660,70,0.8761,0.35224638736248015
19,55660,70,0.874,0.35268692893981934
20,77125,97,0.8672,0.37987245119810104
21,59635,75,0.8612,0.3868491329431534
22,27040,34,0.8658,0.3775331001520157
23,31015,39,0.859,0.3885327780008316
24,28630,36,0.8693,0.3750545871734619
25,27835,35,0.8671,0.37833319187164305
26,29425,37,0.8639,0.3821108981132507
27,33400,42,0.8644,0.3941584250688553
28,33400,42,0.8614,0.38978672432899475
29,40555,51,0.8644,0.3683001462459564
30,39760,50,0.87,0.3613660793662071
31,46120,58,0.8747,0.3592006919145584
32,20680,26,0.8616,0.3878846122264862
33,19090,24,0.8573,0.403090896487236
34,17500,22,0.8593,0.39249164476394655
35,19885,25,0.8607,0.38911584119796755
36,23860,30,0.8624,0.38324350545406344
37,29425,37,0.8525,0.4100404011964798
38,35785,45,0.8601,0.388059646654129
39,44530,56,0.8631,0.3776227922320366
40,39760,50,0.8709,0.35774606132507325
41,43735,55,0.8691,0.3650790519475937
42,37375,47,0.8655,0.37621208454370497
43,44530,56,0.8644,0.3794980634689331
44,44530,56,0.869,0.36857609713077544
45,33400,42,0.866,0.38297931988239287
46,35785,45,0.8535,0.3983852324485779
47,37375,47,0.8695,0.36772182404994963
48,38965,49,0.8712,0.3606053037762642
49,46915,59,0.8695,0.3656547089338303
50,50890,64,0.8683,0.3696380641460419
51,48505,61,0.8681,0.3648059513926506
52,56455,71,0.8646,0.380269304895401
53,71560,90,0.8698,0.3593109187722206
54,53275,67,0.871,0.36333843734264376
55,56455,71,0.8718,0.363441765832901
56,68380,86,0.8703,0.3605102531313896
57,53275,67,0.8739,0.3659502153158188
58,17500,22,0.8523,0.419007905626297
59,21475,27,0.844,0.43235313997268676
60,16705,21,0.8617,0.396038761138916
61,15910,20,0.8538,0.41395915513038634
62,16705,21,0.8576,0.39836058087348936
63,15910,20,0.8575,0.3988452817678452
64,16705,21,0.8573,0.4065467631101608
65,20680,26,0.8633,0.399080757522583
66,23065,29,0.8629,0.38417998846769336
67,17500,22,0.8625,0.3930718137025833
68,15910,20,0.8562,0.4038521536827087
69,11935,15,0.846,0.4322617422103882
70,18295,23,0.8538,0.4059517980098724
71,18295,23,0.8618,0.3994656202077866
